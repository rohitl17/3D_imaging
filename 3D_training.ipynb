{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "RUN For the first time\n",
    "\"\"\"\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "! pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "import math\n",
    "import pydicom\n",
    "import os\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consecutive Slices module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLength(arr):\n",
    " \n",
    "    # intitialize count\n",
    "    count = 0\n",
    "     \n",
    "    # initialize max\n",
    "    result = 0\n",
    " \n",
    "    for i in range(0, len(arr)):\n",
    "     \n",
    "        # Reset count when 0 is found\n",
    "        if (arr[i] == 0):\n",
    "            count = 0\n",
    " \n",
    "        # If 1 is found, increment count\n",
    "        # and update result if count  \n",
    "        # becomes more.\n",
    "        else:\n",
    "             \n",
    "            # increase count\n",
    "            count+= 1\n",
    "            result = max(result, count)  \n",
    "         \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code block for 3D CNN Architecture\n",
    "Returns: V-Net Model\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras_contrib\n",
    "\n",
    "\n",
    "# Building blocks\n",
    "def adding_conv(x, a, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    channel_axis = -1 if data_format=='channels_last' else 1\n",
    "    c = keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, \n",
    "            activation=None, data_format=data_format)(x)\n",
    "    c = keras.layers.add([c, a])\n",
    "    c = keras_contrib.layers.GroupNormalization(groups=groups, axis=channel_axis)(c)\n",
    "    c = keras.layers.advanced_activations.PReLU()(c)\n",
    "    return c\n",
    "\n",
    "def conv(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    channel_axis = -1 if data_format=='channels_last' else 1\n",
    "    c = keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, \n",
    "            activation=None, data_format=data_format)(x)\n",
    "    c = keras_contrib.layers.GroupNormalization(groups=groups, axis=channel_axis)(c)\n",
    "    c = keras.layers.advanced_activations.PReLU()(c)\n",
    "    return c\n",
    "\n",
    "def down_conv(x, filters, kernel_size, padding, data_format, groups):\n",
    "    channel_axis = -1 if data_format=='channels_last' else 1\n",
    "    c = keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=2, \n",
    "                            activation=None, data_format=data_format)(x)\n",
    "    c = keras_contrib.layers.GroupNormalization(groups=groups, axis=channel_axis)(c)\n",
    "    c = keras.layers.advanced_activations.PReLU()(c)\n",
    "    return c\n",
    "\n",
    "def up_conv_concat_conv(x, skip, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    channel_axis = -1 if data_format=='channels_last' else 1\n",
    "    c = keras.layers.Conv3DTranspose(filters, kernel_size=(2,2,2), strides=(2,2,2), \n",
    "                                    data_format=data_format)(x) # up dim(x) by x2\n",
    "    c = keras_contrib.layers.GroupNormalization(groups=groups, axis=channel_axis)(c)\n",
    "    c = keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, \n",
    "                            activation=None, data_format=data_format)(c)\n",
    "    concat = keras.layers.Concatenate(axis=channel_axis)([c, skip]) # concat after Up; dim(skip) == 2*dim(x)\n",
    "    c = keras_contrib.layers.GroupNormalization(groups=groups, axis=channel_axis)(concat)\n",
    "    c = keras.layers.advanced_activations.PReLU()(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "# Encoders\n",
    "def encoder1(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('encoder1'):\n",
    "        with tf.variable_scope('conv'):\n",
    "            conv1 = conv(x, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        with tf.variable_scope('addconv'):\n",
    "            addconv = adding_conv(conv1, conv1, filters, kernel_size, padding, strides, data_format, groups) # N\n",
    "        with tf.variable_scope('downconv'):\n",
    "            downconv = down_conv(addconv, filters*2, kernel_size, padding, data_format, groups) # N/2\n",
    "        return (addconv, downconv)\n",
    "\n",
    "def encoder2(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('encoder2'):\n",
    "        with tf.variable_scope('conv'):\n",
    "            conv1 = conv(x, filters, kernel_size, padding, strides, data_format, groups) \n",
    "        with tf.variable_scope('addconv'):\n",
    "            addconv = adding_conv(conv1, x, filters, kernel_size, padding, strides, data_format, groups) # N/2\n",
    "        with tf.variable_scope('downconv'):\n",
    "            downconv = down_conv(addconv, filters*2, kernel_size, padding, data_format, groups) # N/4\n",
    "        return (addconv, downconv)\n",
    "\n",
    "def encoder3(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('encoder3'):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = conv(x, filters, kernel_size, padding, strides, data_format, groups) # N/4\n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = conv(conv1, filters, kernel_size, padding, strides, data_format, groups) # N/4\n",
    "        with tf.variable_scope('addconv'):\n",
    "            addconv = adding_conv(conv2, x, filters, kernel_size, padding, strides, data_format, groups) # N/4\n",
    "        with tf.variable_scope('downconv'):\n",
    "            downconv = down_conv(addconv, filters*2, kernel_size, padding, data_format, groups) # N/8\n",
    "        return (addconv, downconv)\n",
    "\n",
    "def encoder4(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('encoder4'):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = conv(x, filters, kernel_size, padding, strides, data_format, groups) # N/8\n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = conv(conv1, filters, kernel_size, padding, strides, data_format, groups) # N/8\n",
    "        with tf.variable_scope('addconv'):\n",
    "            addconv = adding_conv(conv2, x, filters, kernel_size, padding, strides, data_format, groups) # N/8\n",
    "        with tf.variable_scope('downconv'):\n",
    "            downconv = down_conv(addconv, filters*2, kernel_size, padding, data_format, groups) # N/16\n",
    "        return (addconv, downconv)\n",
    "\n",
    "\n",
    "# Bottom\n",
    "def bottom(x, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('bottom'):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = conv(x, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = conv(conv1, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        with tf.variable_scope('addconv'):\n",
    "            addconv = adding_conv(conv2, x, filters, kernel_size, padding, strides, data_format, groups) # N/16\n",
    "        return addconv # N/16\n",
    "\n",
    "\n",
    "# Decoders\n",
    "def decoder4(x, skip, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('decoder4'):\n",
    "        with tf.variable_scope('upconv'):\n",
    "            upconv = up_conv_concat_conv(x, skip, filters, kernel_size, padding, strides, data_format, groups) # N/8\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = conv(upconv, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = conv(conv1, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        return conv2 # N/8\n",
    "\n",
    "def decoder3(x, skip, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('decoder3'):\n",
    "        with tf.variable_scope('upconv'):\n",
    "            upconv = up_conv_concat_conv(x, skip, filters, kernel_size, padding, strides, data_format, groups) # N/4\n",
    "        with tf.variable_scope('conv1'):\n",
    "            conv1 = conv(upconv, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        with tf.variable_scope('conv2'):\n",
    "            conv2 = conv(conv1, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        return conv2 # N/4\n",
    "\n",
    "def decoder2(x, skip, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('decoder2'):\n",
    "        with tf.variable_scope('upconv'):\n",
    "            upconv = up_conv_concat_conv(x, skip, filters, kernel_size, padding, strides, data_format, groups) # N/2\n",
    "        with tf.variable_scope('conv'):\n",
    "            conv1 = conv(upconv, filters, kernel_size, padding, strides, data_format, groups)\n",
    "        return conv1 # N/2\n",
    "\n",
    "def decoder1(x, skip, filters, kernel_size, padding, strides, data_format, groups):\n",
    "    with tf.variable_scope('decoder1'):\n",
    "        with tf.variable_scope('upconv'):\n",
    "            upconv = up_conv_concat_conv(x, skip, filters, kernel_size, padding, strides, data_format, groups) # N\n",
    "        return upconv # N\n",
    "\n",
    "\n",
    "# Attention gate\n",
    "def attention_gate(inp, g, intra_filters):\n",
    "    with tf.variable_scope('attention_gate'):\n",
    "        data_format = 'channels_first'##@##\n",
    "        groups = 8 ##@##\n",
    "\n",
    "        # Gating signal processing\n",
    "        g = keras.layers.Conv3D(intra_filters, kernel_size=1, data_format=data_format)(g) # N/2\n",
    "        g = keras_contrib.layers.GroupNormalization(groups=groups, axis=1)(g) # N/2\n",
    "\n",
    "        # Skip signal processing: \n",
    "        x = keras.layers.Conv3D(intra_filters, kernel_size=2, strides=2, padding='same', data_format=data_format)(inp) # N-->N/2\n",
    "        x = keras_contrib.layers.GroupNormalization(groups=groups, axis=1)(x) # N\n",
    "\n",
    "        # Add and proc\n",
    "        g_x = keras.layers.Add()([g, x]) # N/2\n",
    "        psi = keras.layers.Activation('relu')(g_x) # N/2\n",
    "        psi = keras.layers.Conv3D(1, kernel_size = 1, padding='same', data_format=data_format)(psi) # N/2\n",
    "        psi = keras_contrib.layers.GroupNormalization(groups=1, axis=1)(psi) # N/2\n",
    "        psi = keras.layers.Activation('sigmoid')(psi) # N/2\n",
    "        alpha = keras.layers.UpSampling3D(size=2, data_format=data_format)(psi) # N/2-->N\n",
    "\n",
    "\n",
    "        x_hat = keras.layers.Multiply()([inp, alpha])\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "# Model\n",
    "def VNet(n_in, n_out, image_shape, filters, kernel_size, padding, strides, data_format, groups, inter_filters):\n",
    "    with tf.variable_scope('VNet'):\n",
    "        input_dim = image_shape+(n_in,) if data_format=='channels_last' \\\n",
    "            else (n_in,)+image_shape\n",
    "        \n",
    "        inputs = keras.layers.Input(input_dim)\n",
    "\n",
    "        (encoder1_addconv, encoder1_downconv) = encoder1(inputs, filters*2**0, kernel_size, padding, strides, data_format, groups) # N, N/2\n",
    "        (encoder2_addconv, encoder2_downconv) = encoder2(encoder1_downconv, filters*2**1, kernel_size, padding, strides, data_format, groups) # N/2, N/4\n",
    "        (encoder3_addconv, encoder3_downconv) = encoder3(encoder2_downconv, filters*2**2, kernel_size, padding, strides, data_format, groups) # N/4, N/8\n",
    "        (encoder4_addconv, encoder4_downconv) = encoder4(encoder3_downconv, filters*2**3, kernel_size, padding, strides, data_format, groups) # N/8, N/16\n",
    "\n",
    "        bottom_addconv = bottom(encoder4_downconv, filters*2**4, kernel_size, padding, strides, data_format, groups) # N/16\n",
    "\n",
    "        encoder4_ag = attention_gate(encoder4_addconv, bottom_addconv, inter_filters) # (N/8, N/16) --> N/8\n",
    "        decoder4_conv = decoder4(bottom_addconv, encoder4_ag, filters*2**3, kernel_size, padding, strides, data_format, groups) # N/8\n",
    "        encoder3_ag = attention_gate(encoder3_addconv, decoder4_conv, inter_filters) # (N/4, N/8) --> N/4\n",
    "        decoder3_conv = decoder3(decoder4_conv, encoder3_ag, filters*2**2, kernel_size, padding, strides, data_format, groups) # N/4\n",
    "        encoder2_ag = attention_gate(encoder2_addconv, decoder3_conv, inter_filters) # (N/2, N/4) --> N/2\n",
    "        decoder2_conv = decoder2(decoder3_conv, encoder2_ag, filters*2**1, kernel_size, padding, strides, data_format, groups) # N/2\n",
    "        encoder1_ag = attention_gate(encoder1_addconv, decoder2_conv, inter_filters) # (N, N/2) --> N\n",
    "        decoder1_conv = decoder1(decoder2_conv, encoder1_ag, filters*2**0, kernel_size, padding, strides, data_format, groups) # N\n",
    "       \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            outputs = keras.layers.Conv3D(n_out,\n",
    "                (1,1,1),\n",
    "                padding='same',\n",
    "                activation='sigmoid',\n",
    "                data_format=data_format)(decoder1_conv)\n",
    "\n",
    "            model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageSlice Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipyw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ImageSliceViewer3D:\n",
    "    \"\"\" \n",
    "    ImageSliceViewer3D is for viewing volumetric image slices in jupyter or\n",
    "    ipython notebooks. \n",
    "    \n",
    "    User can interactively change the slice plane selection for the image and \n",
    "    the slice plane being viewed. \n",
    "\n",
    "    Argumentss:\n",
    "    Volume = 3D input image\n",
    "    figsize = default(8,8), to set the size of the figure\n",
    "    cmap = default('plasma'), string for the matplotlib colormap. You can find \n",
    "    more matplotlib colormaps on the following link:\n",
    "    https://matplotlib.org/users/colormaps.html\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, volume, figsize=(8,8), cmap='plasma'):\n",
    "        self.volume = volume\n",
    "        self.figsize = figsize\n",
    "        self.cmap = cmap\n",
    "        self.v = [np.min(volume), np.max(volume)]\n",
    "        \n",
    "        # Call to select slice plane\n",
    "        ipyw.interact(self.view_selection, view=ipyw.RadioButtons(\n",
    "            options=['x-y','y-z', 'z-x'], value='x-y', \n",
    "            description='Slice plane selection:', disabled=False,\n",
    "            style={'description_width': 'initial'}))\n",
    "    \n",
    "    def view_selection(self, view):\n",
    "        # Transpose the volume to orient according to the slice plane selection\n",
    "        orient = {\"y-z\":[1,2,0], \"z-x\":[2,0,1], \"x-y\": [0,1,2]}\n",
    "        self.vol = np.transpose(self.volume, orient[view])\n",
    "        maxZ = self.vol.shape[2] - 1\n",
    "        \n",
    "        # Call to view a slice within the selected slice plane\n",
    "        ipyw.interact(self.plot_slice, \n",
    "            z=ipyw.IntSlider(min=0, max=maxZ, step=1, continuous_update=False, \n",
    "            description='Image Slice:'))\n",
    "        \n",
    "    def plot_slice(self, z):\n",
    "        # Plot slice for the given plane and slice\n",
    "        self.fig = plt.figure(figsize=self.figsize)\n",
    "        plt.imshow(self.vol[:,:,z], cmap=plt.get_cmap(self.cmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom ImageSliceViewer3D\n",
    "Arguments:\n",
    "    volume : 3D Volume of CT scan\n",
    "    volume_1 : 3D Volume of Label/True masks\n",
    "    volume_2 : 3D Volume of Predicted masks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import ipywidgets as ipyw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ImageSliceViewer3DMultipleColour:\n",
    "    \"\"\" \n",
    "    ImageSliceViewer3D is for viewing volumetric image slices in jupyter or\n",
    "    ipython notebooks. \n",
    "    \n",
    "    User can interactively change the slice plane selection for the image and \n",
    "    the slice plane being viewed. \n",
    "\n",
    "    Argumentss:\n",
    "    Volume = 3D input image\n",
    "    figsize = default(8,8), to set the size of the figure\n",
    "    cmap = default('plasma'), string for the matplotlib colormap. You can find \n",
    "    more matplotlib colormaps on the following link:\n",
    "    https://matplotlib.org/users/colormaps.html\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, volume, volume_1, volume_2, figsize=(8,8), cmap='plasma'):\n",
    "        self.volume = volume/np.max(volume)\n",
    "        self.volume_1 = volume_1/np.max(volume_1)\n",
    "        self.volume_2 = volume_2/np.max(volume_2)\n",
    "        self.figsize = figsize\n",
    "        self.cmap = cmap\n",
    "        self.v = [np.min(volume), np.max(volume)]\n",
    "        \n",
    "        # Call to select slice plane\n",
    "        ipyw.interact(self.view_selection, view=ipyw.RadioButtons(\n",
    "            options=['x-y','y-z', 'z-x'], value='x-y', \n",
    "            description='Slice plane selection:', disabled=False,\n",
    "            style={'description_width': 'initial'}))\n",
    "    \n",
    "    def view_selection(self, view):\n",
    "        # Transpose the volume to orient according to the slice plane selection\n",
    "        orient = {\"y-z\":[1,2,0], \"z-x\":[2,0,1], \"x-y\": [0,1,2]}\n",
    "        self.vol = np.transpose(self.volume, orient[view])\n",
    "        self.vol_1 = np.transpose(self.volume_1, orient[view])\n",
    "        self.vol_2 = np.transpose(self.volume_2, orient[view])\n",
    "        maxZ = self.vol.shape[2] - 1\n",
    "        \n",
    "        # Call to view a slice within the selected slice plane\n",
    "        ipyw.interact(self.plot_slice, \n",
    "            z=ipyw.IntSlider(min=0, max=maxZ, step=1, continuous_update=False, \n",
    "            description='Image Slice:'))\n",
    "        \n",
    "    def plot_slice(self, z):\n",
    "        # Plot slice for the given plane and slice\n",
    "        self.fig = plt.figure(figsize=self.figsize)\n",
    "        \n",
    "        img_true_mask = cv2.addWeighted(self.vol_1[:,:,z], 0.7, self.vol[:,:,z], 1, 0)\n",
    "        img_pred_mask = cv2.addWeighted(self.vol_2[:,:,z], 0.7, self.vol[:,:,z], 1, 0)\n",
    "#         final_image = np.hstack([self.vol[:,:,z], self.vol_1[:,:,z], self.vol_2[:,:,z]])\n",
    "        final_image = np.hstack([self.vol[:,:,z], img_true_mask, img_pred_mask])\n",
    "        \n",
    "        plt.imshow(final_image, cmap=plt.get_cmap(self.cmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data generator for 3D Volumes:\n",
    "Arguments:\n",
    "    ct_dataframe = Final prepared dataframe\n",
    "    unique_id_list = list of names of unique identifyers (list of strings)\n",
    "    list_num_slices_list = list of number of slices in the scans (list of numbers)\n",
    "    batch_size = batch size (int)\n",
    "    image_size = image size (tuple), eg (512,512)\n",
    "    stack_size = depth of smaller volumes (int)\n",
    "    overlap_size = overlap factor (int)\n",
    "    \n",
    "NOTE: Change the names of the columns according to the dataframe/pathology. \n",
    "      For eg: Change the names wherever there is \"row.\"\n",
    "      Here it is consolidation, predominant_consolidation, ground_glass_opacity, predominant_ground_glass_opacity\n",
    "\"\"\"\n",
    "\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import random\n",
    "class VolDataGenerator3D(Sequence):\n",
    "    \n",
    "    def __init__(self, ct_dataframe, unique_id_list, list_num_slices_list, batch_size, image_size, stack_size, overlap_size):\n",
    "        self.unique_id_list = unique_id_list\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.ct_dataframe = ct_dataframe\n",
    "        self.stack_size = stack_size\n",
    "        self.overlap_size = overlap_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_num_slices_list = list_num_slices_list\n",
    "        \n",
    "        assert(self.overlap_size < self.stack_size)\n",
    "        random.seed(7)\n",
    "        self.list_of_stacks = [(self.unique_id_list[i], current_stack) for i in range(len(self.unique_id_list)) for current_stack in self.make_overlapping_stacks(self.list_num_slices_list[i], self.stack_size, self.overlap_size) ]\n",
    "#         random.shuffle(self.list_of_stacks)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(math.ceil(len(self.list_of_stacks)/self.batch_size))\n",
    "\n",
    "    \n",
    "    def rle_decode(self, mask_rle, shape):\n",
    "        \"\"\"\n",
    "        Return an image array from run-length encoded string `mask_rle` with `shape`.\n",
    "        \"\"\"\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.uint)\n",
    "        if mask_rle==[]:\n",
    "            return np.zeros((shape[0], shape[1]), dtype=np.uint)\n",
    "        else:\n",
    "            s = mask_rle[0].split()\n",
    "            starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "            starts -= 1\n",
    "            ends = starts + lengths\n",
    "\n",
    "            for low, up in zip(starts, ends): img[low:up] = 1\n",
    "        return img.reshape(shape)\n",
    "\n",
    "        \n",
    "    def make_overlapping_stacks(self, num_slices, stack_size, overlap_size):\n",
    "        \"\"\"\n",
    "        Makes the stacks of 3D Volumes for a single CT scan \n",
    "        \n",
    "        Arguments: \n",
    "            num_slices = total number of slcies in a scan (int) \n",
    "            stack_size = stack size of the smaller volume\n",
    "            overlap_size = overlap size\n",
    "        \n",
    "        Returns:\n",
    "            list of indices = eg: ((0,80), (80,160), (160,240), (240,320))\n",
    "        \"\"\"\n",
    "        list_of_indices = [(start, min(start + stack_size, num_slices - 1)) for start in range(0, num_slices, stack_size - overlap_size)]\n",
    "        all_endings = list(map(lambda x: x[1], list_of_indices))\n",
    "        first_index_of_max_end = all_endings.index(max(all_endings))\n",
    "        list_of_indices = list_of_indices[: first_index_of_max_end + 1]\n",
    "        self.list_of_indices = list_of_indices\n",
    "        return list_of_indices\n",
    "\n",
    "\n",
    "    def create_3d_vol(self, name_vs_num_slices_element):\n",
    "        \"\"\"\n",
    "        Creates volumes of X and Y of requried size (image.shape[0], image.shape[1], stack_size)\n",
    "        \n",
    "        Arguments:\n",
    "            name_vs_num_slices_element = For eg: ('ct_name', (0, 100))\n",
    "            \n",
    "        Returns:\n",
    "            img_vol = smaller stack volume of ct scan\n",
    "            label_vol = smaller stack volume of labels/masks\n",
    "        \"\"\"\n",
    "        self.name_vs_num_slices_element = name_vs_num_slices_element\n",
    "        temp_df = self.ct_dataframe.loc[self.ct_dataframe[\"unique_identifier\"] == name_vs_num_slices_element[0]]\n",
    "        temp_df = temp_df.iloc[name_vs_num_slices_element[1][0] : name_vs_num_slices_element[1][1]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        \n",
    "        img_size = (pydicom.read_file(temp_df.iloc[0].dicom_path).pixel_array).shape  #SHAPE OF IMAGE\n",
    "        \n",
    "        img_vol = np.zeros((self.image_size[0], self.image_size[1],  self.stack_size))\n",
    "        label_vol = np.zeros((self.image_size[0], self.image_size[1],  self.stack_size))\n",
    "        \n",
    "        \n",
    "        for i, row in temp_df.iterrows():\n",
    "            img = (pydicom.read_file(row.dicom_path).pixel_array)\n",
    "            img = cv2.resize(img, (self.image_size[0], self.image_size[1]))\n",
    "            img_vol[:,:,i] = img\n",
    "            \n",
    "        unique_unique = (temp_df.combine_pathos.unique()).all()\n",
    "        \n",
    "        for i, row in temp_df.iterrows():\n",
    "\n",
    "            label_con = np.maximum(self.rle_decode(eval(row.bleed), img_size), self.rle_decode(eval(row.bleed), img_size))\n",
    "            label_ggo = np.maximum(self.rle_decode(eval(row.bleed), img_size), self.rle_decode(eval(row.bleed), img_size))\n",
    "            label_whole = np.maximum(label_con, label_ggo)\n",
    "            label = cv2.resize(label_whole.astype(float), (self.image_size[0], self.image_size[1]))\n",
    "            label_vol[:,:,i] = label\n",
    "                \n",
    "        self.temp_df = temp_df\n",
    "        return img_vol, label_vol\n",
    "            \n",
    "\n",
    "    def create_volume_image(self, name_vs_num_slices_element):\n",
    "        \"\"\"\n",
    "        Creates final volume for X and Y of the expanded dimensions to fit the model requirmenets\n",
    "        Arguments:\n",
    "            name_vs_num_slices_element = For eg: ('ct_name', (0, 100))\n",
    "        \"\"\"\n",
    "        img_vol, label_vol = self.create_3d_vol(name_vs_num_slices_element)        \n",
    "        \n",
    "        img_vol = np.expand_dims(img_vol, 0)\n",
    "        label_vol = np.expand_dims(label_vol, 0)\n",
    "        return img_vol, label_vol\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        studies_for_this_batch = self.list_of_stacks[(self.batch_size*index):self.batch_size*(index+1)]\n",
    "        self.studies_for_this_batch = studies_for_this_batch\n",
    "        \n",
    "        X = np.zeros((len(studies_for_this_batch), 1, self.image_size[0], self.image_size[1], self.stack_size))\n",
    "        Y = np.zeros((len(studies_for_this_batch), 1, self.image_size[0], self.image_size[1], self.stack_size))\n",
    "        \n",
    "   \n",
    "        for j,study in enumerate(studies_for_this_batch):\n",
    "            X[j], Y[j] = self.create_volume_image(study)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_slices_list(dataframe, scan_name_list):\n",
    "    \"\"\"\n",
    "    Returns the list of the number of slices in each of the unique identifyer\n",
    "    Arguemtns:\n",
    "        dataframe: the retrived dataframe from Sushrut CT\n",
    "        scan_name_list = list of names of unique identifyers\"\"\"\n",
    "    num_slices_list = []\n",
    "    for name in scan_name_list:\n",
    "        temp_df = dataframe.loc[dataframe.unique_identifier==name]\n",
    "        num_slices_list.append(len(temp_df))\n",
    "    return num_slices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change train_csv accordignly\n",
    "\"\"\"\n",
    "final_train_df = pd.read_csv('./CSV/brain_bleeds/train_platform_ycm.csv')\n",
    "# final_train_df = get_modified_final_csv(\"\", train_list)\n",
    "\n",
    "final_train_df_unique_series_id_list = (final_train_df.unique_identifier.unique())\n",
    "final_train_df_unique_series_id_num_list = find_num_slices_list(final_train_df, final_train_df_unique_series_id_list)\n",
    "assert(len(final_train_df_unique_series_id_list)==len(final_train_df_unique_series_id_num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change val_csv accordignly\n",
    "\"\"\"\n",
    "final_val_df = pd.read_csv('./CSV/brain_bleeds/val_platform_ycm.csv')\n",
    "# final_val_df = get_modified_final_csv(\"\",val_list)\n",
    "\n",
    "final_val_df_unique_series_id_list = (final_val_df.unique_identifier.unique())\n",
    "final_val_df_unique_series_id_num_list = find_num_slices_list(final_val_df, final_val_df_unique_series_id_list)\n",
    "assert(len(final_val_df_unique_series_id_list)==len(final_val_df_unique_series_id_num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change test_csv accordignly\n",
    "\"\"\"\n",
    "final_test_df = pd.read_csv('./CSV/brain_bleeds/test_platform_ycm.csv')\n",
    "# final_test_df = get_modified_final_csv(\"\",test_list)\n",
    "\n",
    "final_test_df_unique_series_id_list = (final_test_df.unique_identifier.unique())\n",
    "final_test_df_unique_series_id_num_list = find_num_slices_list(final_test_df, final_test_df_unique_series_id_list)\n",
    "assert(len(final_test_df_unique_series_id_list)==len(final_test_df_unique_series_id_num_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# END*****\n",
    "print(len(final_train_df.unique_identifier.unique()),len(final_val_df.unique_identifier.unique()),len(final_test_df.unique_identifier.unique()),len(final_out_test_df.unique_identifier.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters for the Data Generator\n",
    "\"\"\"\n",
    "image_size = (512, 512)\n",
    "stack_size = 32\n",
    "overlap_size = 20\n",
    "batch_size = 1\n",
    "assert(overlap_size < stack_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df_gen = VolDataGenerator3D(ct_dataframe = final_train_df,\n",
    "                               unique_id_list = final_train_df_unique_series_id_list,\n",
    "                               list_num_slices_list = final_train_df_unique_series_id_num_list,\n",
    "                               batch_size = batch_size,\n",
    "                               image_size = image_size,\n",
    "                               stack_size = stack_size,\n",
    "                               overlap_size = overlap_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_val_df_gen = VolDataGenerator3D(ct_dataframe = final_val_df,\n",
    "                               unique_id_list = final_val_df_unique_series_id_list,\n",
    "                               list_num_slices_list = final_val_df_unique_series_id_num_list,\n",
    "                               batch_size = batch_size,\n",
    "                               image_size = image_size,\n",
    "                               stack_size = stack_size,\n",
    "                               overlap_size = overlap_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df_gen = VolDataGenerator3D(ct_dataframe = final_test_df,\n",
    "                               unique_id_list = final_test_df_unique_series_id_list,\n",
    "                               list_num_slices_list = final_test_df_unique_series_id_num_list,\n",
    "                               batch_size = batch_size,\n",
    "                               image_size = image_size,\n",
    "                               stack_size = stack_size,\n",
    "                               overlap_size = overlap_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=80\n",
    "ImageSliceViewer3DMultipleColour(np.squeeze(final_train_df_gen[x][0]), np.squeeze(final_train_df_gen[x][1]), np.squeeze(final_train_df_gen[x][1]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(final_train_df_gen[80][0][0])[:,:,23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing V-Net\n",
    "\"\"\"\n",
    "\n",
    "image_shape = (image_size[0], image_size[1], stack_size)\n",
    "group_size = 2\n",
    "f_root = 8\n",
    "filters = 4\n",
    "model = VNet(image_shape=image_shape, n_in=1, n_out=1, \n",
    "        strides=1, padding='same', kernel_size=3,\n",
    "        groups=group_size, data_format='channels_first',\n",
    "        inter_filters=f_root, filters = filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model compile\n",
    "\"\"\"\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.reshape(y_pred, (-1, 2))\n",
    "    intersection = K.mean(y_true_f * y_pred_f[:,0]) + K.mean((1.0 - y_true_f) * y_pred_f[:,1])\n",
    "    \n",
    "    return 2. * intersection;\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "smooth = 1\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f1 = K.flatten(K.round(y_pred))\n",
    "    intersection = K.sum(y_true_f * y_pred_f1)\n",
    "    return (2. * intersection) / (K.sum(y_true_f) + K.sum(y_pred_f1) + smooth)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy', dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Model checkpoints and lr scheduler\n",
    "\"\"\"\n",
    "\n",
    "model_checkpoint_path = '/opt/bucketdata/Users/Rohit/3D_CT_Model/'\n",
    "\n",
    "model_checkpoint_1 = ModelCheckpoint(os.path.join(model_checkpoint_path, 'Weights', 'brain_bleeds_loss_platform_ycm_pretrained_covid.hdf5'),\n",
    "                                     monitor='val_loss', mode='min',save_best_only=True, verbose=1)\n",
    "\n",
    "# model_checkpoint_2 = ModelCheckpoint(os.path.join(model_checkpoint_path, 'Weights', 'brain_bleeds_1_dice.hdf5'),\n",
    "#                                      monitor='val_dice_coef', mode='max',save_best_only=True, verbose=1)\n",
    "\n",
    "csvlogger = CSVLogger(os.path.join(model_checkpoint_path, 'CSV_Log', 'brain_bleeds_platform_ycm_pretrained.csv'))\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch  % 4 == 0 and epoch != 0:\n",
    "        return lr / 3\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "callbacks = [model_checkpoint_1, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load previous weights if any\n",
    "\"\"\"\n",
    "\n",
    "model_checkpoint_path_abhishek = '/opt/bucketdata/Users/Abhishek/3D_SEGMENTATION_COMPARE_23_NOV/stats_&_weights'\n",
    "model.load_weights(os.path.join(model_checkpoint_path_abhishek, 'Weights', 'minvalloss_seg_3d_UNIQUE_INDENTIFIER_BATCH_APPROACH_trial_6_512X32_20overlap.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start the training\n",
    "\"\"\"\n",
    "model.fit_generator(final_train_df_gen,\n",
    "                    validation_data=final_val_df_gen,\n",
    "                    epochs=30,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.read_csv(os.path.join(model_checkpoint_path, 'CSV_Log', 'seg_3d_UNIQUE_INDENTIFIER_BATCH_APPROACH_trial_4_352X80_0overlap.csv')).val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df_preds = model.predict_generator(final_test_df_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageSliceViewer3DMultipleColour(np.squeeze(final_test_df_gen[4][0]), np.squeeze(final_test_df_gen[4][1]), (np.squeeze(final_test_df_preds[4]>0.08)), cmap='gray', figsize=(8,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
